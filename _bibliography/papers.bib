---
---

@string{aps = {American Physical Society,}}



@inproceedings{zaidi_2023,
	title = {The La Forge Speech Synthesis System for Blizzard Challenge 2023},
	url = {https://www.isca-archive.org/blizzard_2023/zaidi23_blizzard.pdf},
  abstract = {This paper describes the La Forge entry to the Blizzard Challenge of 2023 focusing on text-to-speech in French and homograph disambiguation. Our system is based on VAE-Tacotron and HiFi-GAN. We implement several improvements on the baseline models such as a cycle consistency loss for better style modeling, a style reference selection method to improve overall naturalness and an over-produce and select method that chooses the best synthesized candidate across multiple variations using automatic speech recognition. We also build a linguistic frontend capable of homograph disambiguation using part-of-speech tagging and simple rules. We publicly release our hand annotated data set for French homograph disambiguation1. Results from subjective listening tests show the effectiveness of our system in disambiguating homographs and generating high-quality synthetic speech.},
	booktitle = {18th Blizzard Challenge Workshop - INTERSPEECH},
	author = {Zaïdi, Julian and Duchêne, Corentin and Seuté, Hugo and Carbonneau, Marc-André},
	year = {2023},
	bibtex_show={true},
  selected={false},
}


@inproceedings{josi_2024,
	title = {SEREP: Semantic Facial Expression Representation for Robust In-the-Wild Capture and Retargeting},
	url = {https://arxiv.org/pdf/2412.14371},
  abstract = {Monocular facial performance capture in-the-wild is challenging due to varied capture conditions, face shapes, and expressions. Most current methods rely on linear 3D Morphable Models, which represent facial expressions independently of identity at the vertex displacement level. We propose SEREP (Semantic Expression Representation), a model that disentangles expression from identity at the semantic level. It first learns an expression representation from unpaired 3D facial expressions using a cycle consistency loss. Then we train a model to predict expression from monocular images using a novel semi-supervised scheme that relies on domain adaptation. In addition, we introduce MultiREX, a benchmark addressing the lack of evaluation resources for the expression capture task. Our experiments show that SEREP outperforms state-of-the-art methods, capturing challenging expressions and transferring them to novel identities.},
	booktitle = {ArXiv},
	author = {Josi, Arthur and Hafemann, Luiz Gustavo and Dib, Abdallah and Got, Emeline and Cruz, Rafael MO and Carbonneau, Marc-André},
	year = {2025},
	bibtex_show={true},
  preview={serep.png},
  selected={false},
}

@inproceedings{Fadaeinejad2025,
  bibtex_show={true},
  author={Fadaeinejad, Amin and Dib, Abdallah and Hafemann, {Luiz Gustavo} and Got, Emeline and Anderson, Trevor and Depierre, Amaury and Troje, Nikolaus F. and Brubaker, Marcus A and and Carbonneau, Marc-André},
  abstract={Fulfilling a precise artistic vision while creating realistic virtual human characters requires extensive manual efforts. This paper proposes a novel approach to streamline this process, generating 3D head geometry and enabling precise control over skin tone and fine-grained modification of facial details such as wrinkles. User-specified modifications are conveniently propagated over the entire assets by our models, effectively reducing the amount of manual intervention needed to achieve a specific artistic vision. This is achieved by our proposed texture-generation pipeline that leverages correlations between texture and geometry for different head shapes, ethnicity, and gender. Our method allows for accurate skin-tone control while keeping the other appearance factors unchanged. Lastly, we introduce a method for fine-grained control over the details of the generated heads, which enables artists to freely modify one texture map and have changes cohesively propagated to the other maps. Our experiments show that our method produces diverse and well-behaved geometries, thanks to our GNN-based model, and synthesizes textures that are coherent with the geometry using a CNN-based GAN. We also achieve precise and intuitive skin-tone control through a single control parameter and obtain plausible textures for both face skin and lips. Our experiments with fine-grained editing on common artists' tasks, such as adding wrinkles or removing a beard, showcase how our method simplifies the head generation workflow by cohesively propagating changes to all texture maps.},
  booktitle = {6th AI for Creative Visual Content Generation Editing and Understanding (CVEU) - CVPR},
  title={Geometry-Aware Texture Generation for 3D Head Modeling with Artist-driven Control}, 
  year={2025},
  selected={false}
}


@inproceedings{vanniekerk2024WD,
  bibtex_show={true},
  author={{van Niekerk}, Benjamin and Zaïdi, Julian and Carbonneau, Marc-André and Kamper, Herman},
  abstract={Discovering a lexicon from unlabeled audio is a longstanding challenge for zero-resource speech processing. One approach is to search for frequently occurring patterns in speech. We revisit this idea by proposing DUSTED: Discrete Unit Spoken-TErm Discovery. Leveraging self-supervised models, we encode input audio into sequences of discrete units. Inspired by alignment algorithms from bioinformatics, we find repeated speech patterns by searching for similar sub-sequences of units. Since discretization discards speaker information, DUSTED finds better matches across speakers, improving the coverage and consistency of the discovered patterns. We demonstrate these improvements on the ZeroSpeech Challenge, achieving state-of-the-art results on the spoken-term discovery track. Finally, we analyze the duration distribution of the patterns, showing that our method finds longer word- or phrase-like terms.},
  booktitle = {{INTERSPEECH}},
  title={Spoken-Term Discovery using Discrete Speech Units}, 
  year={2024},
  preview={word_disc.png},
  selected={true}
}

@inproceedings{Davoodnia2024,
  bibtex_show={true},
  author={Davoodnia, Vandad and Ghorbani, Saeed and Carbonneau, Marc-André and Messier, Alexandre and Etemad, Ali},
  abstract={We introduce UPose3D, a novel approach for multi-view 3D human pose estimation, addressing challenges in accuracy and scalability. Our method advances existing pose estimation frameworks by improving robustness and flexibility without requiring direct 3D annotations. At the core of our method, a pose compiler module refines predictions from a 2D keypoints estimator that operates on a single image by leveraging temporal and cross-view information. Our novel cross-view fusion strategy is scalable to any number of cameras, while our synthetic data generation strategy ensures generalization across diverse actors, scenes, and viewpoints. Finally, UPose3D leverages the prediction uncertainty of both the 2D keypoint estimator and the pose compiler module. This provides robustness to outliers and noisy data, resulting in state-of-the-art performance in out-of-distribution settings. In addition, for in-distribution settings, UPose3D yields a performance rivaling methods that rely on 3D annotated data, while being the state-of-the-art among methods relying only on 2D supervision.},
  booktitle = {ECCV},
  title={UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues}, 
  year={2024},
  preview={markerless.png},
  selected={true}
}


@inproceedings{Dib2023,
  bibtex_show={true},
  author={Dib, Abdallah and Hafemann, {Luiz Gustavo} and Got, Emeline and Anderson, Trevor and Fadaeinejad, Amin and Cruz {Rafael M. O.}and Carbonneau, Marc-André},
  abstract={Reconstructing an avatar from a portrait image has many applications in multimedia, but remains a challenging research problem. Extracting reflectance maps and geometry from one image is ill-posed: recovering geometry is a one-to-many mapping problem and reflectance and light are difficult to disentangle. Accurate geometry and reflectance can be captured under the controlled conditions of a light stage, but it is costly to acquire large datasets in this fashion. Moreover, training solely with this type of data leads to poor generalization with in-the-wild images. This motivates the introduction of MoSAR, a method for 3D avatar generation from monocular images. We propose a semi-supervised training scheme that improves generalization by learning from both light stage and in-the-wild datasets. This is achieved using a novel differentiable shading formulation. We show that our approach effectively disentangles the intrinsic face parameters, producing relightable avatars. As a result, MoSAR estimates a richer set of skin reflectance maps, and generates more realistic avatars than existing state-of-the-art methods.},
  booktitle = {{CVPR}},
  title={MoSAR: Monocular Semi-Supervised Model For Avatar Reconstruction Using Differentiable Shading}, 
  year={2024},
  preview={mosar.png},
  selected={true}
}

@inproceedings{Lopez2024align,
  bibtex_show={true},
  author={{Lopez Latouche}, Gaëtan and Carbonneau, Marc-André and Swanson, Ben},
  abstract={Real world deployments of word alignment are almost certain to cover both high and low resource languages.  However, the state-of-the-art for this task recommends a different model class depending on the availability of gold alignment training data for a particular language pair.  We propose BinaryAlign, a novel word alignment technique based on binary sequence labeling that outperforms existing approaches in both scenarios, offering a unifying approach to the task.   Additionally, we vary the specific choice of multilingual foundation model, perform stratified error analysis over alignment error type, and explore the performance of BinaryAlign on non-English language pairs. We make our source code publicly available.},
  booktitle = {ACL},
  title={BinaryAlign: Word Alignment as Binary Sequence Labeling}, 
  year={2024},
  preview={binalign.png},
  selected={true}
}

@inproceedings{Lopez2024ZCLT,
  bibtex_show={true},
  author={{Lopez Latouche}, Gaëtan and Carbonneau, Marc-André and Swanson, Ben},
  abstract={Grammatical Error Detection (GED) methods rely heavily on human annotated error corpora. However, these annotations are unavailable in many low-resource languages. In this paper, we investigate GED in this context. Leveraging the zero-shot cross-lingual transfer capabilities of multilingual pre-trained language models, we train a model using data from a diverse set of languages to generate synthetic errors in other languages. These synthetic error corpora are then used to train a GED model. Specifically we propose a two-stage fine-tuning pipeline where the GED model is first fine-tuned on multilingual synthetic data from target languages followed by fine-tuning on human-annotated GED corpora from source languages. This approach outperforms current state-of-the-art annotation-free GED methods. We also analyse the errors produced by our method and other strong baselines, finding that our approach produces errors that are more diverse and more similar to human errors.},
  booktitle = {EMNLP},
  title={Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection}, 
  year={2024},
  preview={ZCLT.png},
  selected={false}
}


@inproceedings{zhu_2022,
  title={EDMSound: Spectrogram Based Diffusion Models for Efficient and High-Quality Audio Synthesis},
  author={Zhu, Ge and Wen, Yutong and Carbonneau, Marc-Andr{\'e} and Duan, Zhiyao},
  abstract={Audio diffusion models can synthesize a wide variety of sounds. Existing models often operate on the latent domain with cascaded phase recovery modules to reconstruct waveform. This poses challenges when generating high-fidelity audio. In this paper, we propose EDMSound, a diffusion-based generative model in spectrogram domain under the framework of elucidated diffusion models (EDM). Combining with efficient deterministic sampler, we achieved similar Fréchet audio distance (FAD) score as top-ranked baseline with only 10 steps and reached state-of-the-art performance with 50 steps on the DCASE2023 foley sound generation benchmark. We also revealed a potential concern regarding diffusion based audio generation models that they tend to generate samples with high perceptual similarity to the data from training data.},
  booktitle = {{NeurIPS Workshop: Machine Learning for Audio}},
  year = {2023},
	bibtex_show={true},
  selected={true},
  preview={edm.png}
}


@ARTICLE{Carbonneau2022,
  bibtex_show={true},
  author={Carbonneau, Marc-André and Zaïdi, Julian and Boilard, Jonathan and Gagnon, Ghyslain},
  abstract={Learning to disentangle and represent factors of variation in data is an important problem in AI. While many advances have been made to learn these representations, it is still unclear how to quantify disentanglement. While several metrics exist, little is known on their implicit assumptions, what they truly measure, and their limits. In consequence, it is difficult to interpret results when comparing different representations. In this work, we survey supervised disentanglement metrics and thoroughly analyze them. We propose a new taxonomy in which all metrics fall into one of three families: intervention-based, predictor-based and information-based. We conduct extensive experiments in which we isolate properties of disentangled representations, allowing stratified comparison along several axes. From our experiment results and analysis, we provide insights on relations between disentangled representation properties. Finally, we share guidelines on how to measure disentanglement.},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Measuring Disentanglement: A Review of Metrics}, 
  year={2022},
  doi={10.1109/TNNLS.2022.3218982},
  preview={disentangle.png},
  selected={true}
}

@ARTICLE{vanniekerk2023rhythm,
  author={{van Niekerk}, Benjamin and Carbonneau, Marc-André and Kamper, Herman},
  journal={IEEE Signal Processing Letters}, 
  title={Rhythm Modeling for Voice Conversion}, 
  year={2023},
  volume={30},
  number={},
  pages={1297-1301},
  preview={Urhythmic.png},
  bibtex_show={true},
  selected={true},
  abstract={Voice conversion aims to transform source speech into a different target voice. However, typical voice conversion systems do not account for rhythm, which is an important factor in the perception of speaker identity. To bridge this gap, we introduce Urhythmic—an unsupervised method for rhythm conversion that does not require parallel data or text transcriptions. Using self-supervised representations, we first divide source audio into segments approximating sonorants, obstruents, and silences. Then we model rhythm by estimating speaking rate or the duration distribution of each segment type. Finally, we match the target speaking rate or rhythm by time-stretching the speech segments. Experiments show that Urhythmic outperforms existing unsupervised methods in terms of quality and prosody.},
  doi={10.1109/LSP.2023.3313515}
  }




@article{langevin_energy_2021,
	title = {Energy {Disaggregation} using {Variational} {Autoencoders}},
	volume = {254},
	issn = {03787788},
	doi = {10.1016/j.enbuild.2021.111623},
	journal = {Energy \& Buildings},
	author = {Langevin, Antoine and Carbonneau, Marc-André and Cheriet, Mohamed and Gagnon, Ghyslain},
	year = {2021},
	keywords = {non-intrusive load monitoring, energy disaggregation, Energy disaggregation, Generative models, nilm, Non-intrusive load monitoring (NILM), vae, variational autoencoders, Variational autoencoders (VAE)},
	pages = {111623},
	bibtex_show={true}
}

@inproceedings{van_niekerk_comparaison_2022,
	title = {A Comparaison of Discrete and Soft Speech Units for Improved Voice Conversion},
	isbn = {978-1-66540-540-9},
	booktitle = {{ICASSP}},
  abstract={The goal of voice conversion is to transform source speech into a target voice, keeping the content unchanged. In this paper, we focus on self-supervised representation learning for voice conversion. Specifically, we compare discrete and soft speech units as input features. We find that discrete representations effectively remove speaker information but discard some linguistic content - leading to mispronunciations. As a solution, we propose soft speech units. To learn soft units, we predict a distribution over discrete speech units. By modeling uncertainty, soft units capture more content information, improving the intelligibility and naturalness of converted speech.},
	author = {{van Niekerk}, Benjamin and Carbonneau, Marc-André and Zaidi, Julian and Baas, Matthew and Seuté, Hugo and Kamper, Herman},
	year = {2022},
	pages = {6562--6566},
	bibtex_show={true},
  selected={true},
  preview={svc2.png},
}

@inproceedings{zaidi_2022,
	title = {Daft-Exprt: Robust Prosody Transfer Across Speakers for Expressive Speech Synthesis},
	url = {http://arxiv.org/abs/2108.02271},
	booktitle = {INTERSPEECH},
	author = {Zaïdi, Julian and Seuté, Hugo and {van Niekerk}, Benjamin and Carbonneau, Marc-André},
	year = {2022},
	bibtex_show={true},
  selected={true},
  preview={daft.png},
}

@inproceedings{ghorbani_exemplar-based_2022,
	title = {Exemplar-based Stylized Gesture Generation from Speech: An Entry to the GENEA Challenge 2022},
	isbn = {978-1-4503-9390-4},
	url = {https://dl.acm.org/doi/10.1145/3536221.3558068},
	doi = {10.1145/3536221.3558068},
	booktitle = {International Conference on Multimodal Intreraction},
	publisher = {ACM},
	author = {Ghorbani, Saeed and Ferstl, Ylva and Carbonneau, Marc-André},
	year = {2022},
	pages = {778--783},
	bibtex_show={true},
}

@ARTICLE{Carbonneau2020_personality,
  author={Carbonneau, Marc-André and Granger, Eric and Attabi, Yazid and Gagnon, Ghyslain},
  journal={IEEE Transactions on Affective Computing}, 
  title={Feature Learning from Spectrograms for Assessment of Personality Traits}, 
  year={2020},
  volume={11},
  number={1},
  pages={25-31},
  bibtex_show={true},
  doi={10.1109/TAFFC.2017.2763132}}

@ARTICLE{carbonneau_bag-level_2019,
  author={Carbonneau, Marc-André and Granger, Eric and Gagnon, Ghyslain},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Bag-Level Aggregation for Multiple-Instance Active Learning in Instance Classification Problems}, 
  year={2019},
  volume={30},
  number={5},
  pages={1441-1451},
  bibtex_show={true},
  doi={10.1109/TNNLS.2018.2869164}}


@INPROCEEDINGS{carbonneau_ipta2016,
  author={Carbonneau, Marc-André and Granger, Eric and Gagnon, Ghyslain},
  booktitle={2016 Sixth International Conference on Image Processing Theory, Tools and Applications (IPTA)}, 
  title={Score thresholding for accurate instance classification in multiple instance learning}, 
  year={2016},
  bibtex_show={true},
  doi={10.1109/IPTA.2016.7821026}}


@inproceedings{Carbonneau2015_hockey,
	title = {Real-time visual play-break detection in sport events using a context descriptor},
	doi = {10.1109/ISCAS.2015.7169270},
	booktitle = {Circuits and {Systems} ({ISCAS}), 2015 {IEEE} {International} {Symposium} on},
	author = {Carbonneau, Marc-André and Raymond, Alexandre J. and Granger, Eric and Gagnon, Ghyslain},
	month = may,
	year = {2015},
	bibtex_show={true},
	pages = {2808--2811},
}


@article{Carbonneau2016Survey,
title = {Multiple instance learning: A survey of problem characteristics and applications},
journal = {Pattern Recognition},
volume = {77},
pages = {329-353},
year = {2018},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2017.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S0031320317304065},
author = {Marc-André Carbonneau and Veronika Cheplygina and Eric Granger and Ghyslain Gagnon},
keywords = {Multiple instance learning, Weakly supervised learning, Classification, Multi-instance learning, Computer vision, Computer aided diagnosis, Document classification, Drug activity prediction},
bibtex_show={true},
selected={true},
}


@article{CARBONNEAU_RSIS,
title = {Robust multiple-instance learning ensembles using random subspace instance selection},
journal = {Pattern Recognition},
volume = {58},
pages = {83-99},
year = {2016},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2016.03.035},
url = {https://www.sciencedirect.com/science/article/pii/S0031320316300346},
author = {Marc-André Carbonneau and Eric Granger and Alexandre J. Raymond and Ghyslain Gagnon},
keywords = {Multiple-instance learning, Random subspace methods, Classifier ensembles, Instance selection, Weakly supervised learning, Classification, MIL},
bibtex_show={true},
}

@article{CARBONNEAU_ALARM,
title = {Detection of alarms and warning signals on an digital in-ear device},
journal = {International Journal of Industrial Ergonomics},
volume = {43},
number = {6},
pages = {503-511},
year = {2013},
note = {Noise: Assessment & Control},
issn = {0169-8141},
doi = {https://doi.org/10.1016/j.ergon.2012.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169814112000625},
author = {Marc-André Carbonneau and Narimène Lezzoum and Jérémie Voix and Ghyslain Gagnon},
keywords = {Pattern recognition, Digital signal processing, Hearing protection devices, Industrial worker safety},
bibtex_show={true},
}

@INPROCEEDINGS{Carbonneau_icpr,
  author={Carbonneau, Marc-André and Granger, Eric and Gagnon, Ghyslain},
  booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)}, 
  title={Witness identification in multiple instance learning using random subspaces}, 
  year={2016},
  volume={},
  number={},
  pages={3639-3644},
  bibtex_show={true},
  doi={10.1109/ICPR.2016.7900199}}


@article{ZeroEGGS,
author = {Ghorbani, Saeed and Ferstl, Ylva and Holden, Daniel and Troje, Nikolaus F. and Carbonneau, Marc-André},
title = {ZeroEGGS: Zero-shot Example-based Gesture Generation from Speech},
journal = {Computer Graphics Forum},
volume = {42},
number = {1},
pages = {206-216},
keywords = {animation, gestures, character control, motion capture},
doi = {https://doi.org/10.1111/cgf.14734},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14734},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.14734},
bibtex_show={true},
year = {2023},
preview={zeggs.png},
selected={true}

} 

@INPROCEEDINGS{blow,
  author={Carbonneau, Marc-André and Gagnon, Ghyslain and Sabourin, Robert and Dubois, Jean},
  booktitle={2013 IEEE 11th International New Circuits and Systems Conference (NEWCAS)}, 
  title={Recognition of blowing sound types for real-time implementation in mobile devices}, 
  year={2013},
  pages={1-4},
  bibtex_show={true},
  doi={10.1109/NEWCAS.2013.6573655}}

@article{MURPHY2021268,
title = {Artist guided generation of video game production quality face textures},
journal = {Computers & Graphics},
volume = {98},
pages = {268-279},
year = {2021},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2021.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0097849321001199},
author = {Christian Murphy and Sudhir Mudur and Daniel Holden and Marc-André Carbonneau and Donya Ghafourzadeh and Andre Beauchamp},
keywords = {Face texture generation, Artist guided, SuperResolution, BRDF recovery},
bibtex_show={true},
}

@inproceedings{10.1145/3424636.3426898,
author = {Murphy, Christian and Mudur, Sudhir and Holden, Daniel and Carbonneau, Marc-Andr\'{e} and Ghafourzadeh, Donya and Beauchamp, Andre},
title = {Appearance Controlled Face Texture Generation for Video Game Characters},
year = {2020},
isbn = {9781450381710},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424636.3426898},
doi = {10.1145/3424636.3426898},
abstract = {Manually creating realistic, digital human heads is a difficult and time-consuming task for artists. While 3D scanners and photogrammetry allow for quick and automatic reconstruction of heads, finding an actor who fits specific character appearance descriptions can be difficult. Moreover, modern open-world videogames feature several thousands of characters that cannot realistically all be cast and scanned. Therefore, researchers are investigating generative models to create heads fitting a specific character appearance description. While current methods are able to generate believable head shapes quite well, generating a corresponding high-resolution and high-quality texture which respects the character’s appearance description is not possible using current state of the art methods. This work presents a method that generates synthetic face textures under the following constraints: (i) there is no reference photograph to build the texture, (ii) game artists control the generative process by providing precise appearance attributes, the face shape, and the character’s age and gender, and (iii) the texture must be of adequately high resolution and look believable when applied to the given face shape. Our method builds upon earlier deep learning approaches addressing similar problems. We propose several key additions to these methods to be able to use them in our context, specifically for artist control and small training data. In spite of training with a limited amount of training data, just over 100 samples, our model produces realistic textures which comply to a diverse range of skin, hair, lip and iris colors specified through our intuitive description format and augmentation thereof.},
booktitle = {13th ACM SIGGRAPH Conference on Motion, Interaction and Games},
articleno = {9},
numpages = {11},
keywords = {artist controlled character creation, image-to-image translation, face texture generation, fine facial features},
location = {Virtual Event, SC, USA},
series = {MIG '20},
preview={facemig.png},
bibtex_show={true},
selected={false},
}
